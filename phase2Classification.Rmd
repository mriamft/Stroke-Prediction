---
editor_options: 
  markdown: 
    wrap: 72
---

```{r}
data<-read.csv("Dataset/healthcare-dataset-stroke-data.csv")
```

```{r}
# import necessary libraries
library(tidyverse)   
library(ggplot2)
library(lubridate)
library(magrittr)
library(dplyr)
library(tidyr) 
library(readr)
library(outliers)
library(caret) 
library(DMwR2) 
library(tidyverse)
library(lubridate)
library(skimr)
library(janitor)
library(dplyr)
library(tidyr)
library(readr)
library(ggplot2)
library(gridExtra)
```

Cleaning:

```{r}
data$bmi[data$bmi=="N/A"] <-NA
sum(is.na(data))
# Converting bmi to numeric
data$bmi <- as.numeric(as.character(data$bmi))
# Checking bmi type
class(data$bmi)
# Replacing null values with the mean
data$bmi[is.na(data$bmi)]<-mean(data$bmi, na.rm = TRUE)

#Remove outliers:
#install outliers package
library(outliers)
#detect Age outliers
OutAge <- outlier(data$age)
print(OutAge)
#checking the outlier location before delete
indices <- which(data$age == OutAge)
# Print the resulting row indices
print(indices)
#Remove age outlier
data <- data[data$age != OutAge, ]
#detect Average glucose level outliers
OutAvg <- outlier(data$avg_glucose_level)
print(OutAvg)
#Remove Average glucose level outlier
data <- data[data$avg_glucose_level != OutAvg, ]
#detect bmi outliers
OutBMI <- outlier(data$bmi)
print(OutBMI)
#Remove bmi outlier
data <- data[data$bmi != OutBMI, ]
#only one column with Gender "Other" 
data[data$gender=="Other", ]
# delete it 
data = data[data$gender!="Other", ]

#normalize data
normalize <- function(x){ return ((x - min(x))/ (max(x)- min(x)))}
data$avg_glucose_level= normalize(data$avg_glucose_level)
data$age= normalize(data$age)
data$bmi= normalize(data$bmi)
head(data)

#delete id coloumn
data <-data[,!names(data) %in% c("id", "gender", "Residence_type")]
head(data)

# down the data
library('caret')
library('dplyr')
data$stroke <- as.factor(data$stroke)
data<-upSample(data[,-9],data$stroke, yname="stroke")
#data<-downSample(data[,-9],data$stroke, yname="stroke")
plot(data$stroke)
data$stroke<- as.factor(data$stroke)
data <- data %>%group_by(stroke) %>% sample_n(size=500)
```

Data Mining Techniques:

5.1 Classification:

We will choose the attributes with the highest importance (from feature
selection) to create comprehensible tree:

```{r}
myFormula <- stroke ~ age + hypertension +ever_married+avg_glucose_level
```

-Gain ratio (C.50): Gain ratio is a metric used in decision tree
algorithms to evaluate the quality of a split based on the information
gain and the intrinsic information of a feature. It takes into account
the entropy or impurity of a dataset and the potential information
gained by splitting the data based on a specific feature.

The gain ratio is calculated by dividing the information gain by the
split information. Information gain measures the reduction in entropy
achieved by splitting the dataset based on a particular feature. Split
information quantifies the potential information generated by the
feature itself.

Using the gain ratio in a decision tree, the algorithm compares the gain
ratios of different features and selects the feature with the highest
ratio as the best split. This approach helps prevent bias towards
features with a large number of values or categories.

1-partition the data into ( 70% training, 30% testing):

```{r}
#splitting 70% of data for training, 30% of data for testing
library('C50')
set.seed(1234)
ind=sample (2, nrow(data), replace=TRUE, prob=c(0.70 , 0.30))
trainData=data[ind==1,]
testData=data[ind==2,]

#train using the trainData and create the c5.0 gain ratio tree
strokeTree <- C5.0(myFormula, data=trainData)
summary(strokeTree)
plot(strokeTree)
```

```{r}
#make predictions using the c5.0 gain ratio tree on the test data
testPred <- predict(strokeTree, newdata = testData)
table(testPred, testData$stroke)
```

```{r}
##create confusion matrix to evaluate the model's performance
library(caret)
results <- confusionMatrix(testPred, testData$stroke)
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
```

2-partition the data into ( 80% training, 20% testing):

```{r}
#splitting 80% of data for training, 20% of data for testing
set.seed(1234)
ind=sample (2, nrow(data), replace=TRUE, prob=c(0.80 , 0.20))
trainData=data[ind==1,]
testData=data[ind==2,]

#train using the trainData and create the c5.0 gain ratio tree
strokeTree <- C5.0(myFormula, data=trainData)
summary(strokeTree)
plot(strokeTree)
```

```{r}
#make predictions using the c5.0 gain ratio tree on the test data
testPred <- predict(strokeTree, newdata = testData)
table(testPred, testData$stroke)
```

```{r}
#create confusion matrix to evaluate the model's performance
library(caret)
results <- confusionMatrix(testPred, testData$stroke)
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
```

3-partition the data into ( 85% training, 15% testing):

```{r}
#splitting 85% of data for training, 15% of data for testing
trctrl <- trainControl(method = "cv", number = 10, savePredictions=TRUE)
set.seed(1234)
ind=sample (2, nrow(data), replace=TRUE, prob=c(0.85 , 0.15))
trainData=data[ind==1,]
testData=data[ind==2,]

#train using the trainData and create the c5.0 gain ratio tree
strokeTree <- C5.0(myFormula, data=trainData,)
summary(strokeTree)
plot(strokeTree)
```

```{r}
#make predictions using the c5.0 gain ratio tree on the test data
testPred <- predict(strokeTree, newdata = testData)
table(testPred, testData$stroke)
```

```{r}
#create confusion matrix to evaluate the model's performance
library(caret)
results <- confusionMatrix(testPred, testData$stroke)
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
```

-Information gain (ctree) Information gain is a measure used in decision
tree algorithms to evaluate the usefulness of a feature in splitting the
data. It quantifies the reduction in entropy or impurity achieved by
splitting the dataset based on that feature.

Entropy is a measure of the disorder or randomness in a dataset,
specifically the uncertainty of class labels. Information gain
calculates the difference between the entropy of the parent node (before
the split) and the weighted average of the entropies of the child nodes
(after the split).

To use information gain in a decision tree, the algorithm examines
different features and calculates the information gain for each one. The
feature with the highest information gain is selected as the best choice
for splitting the data.

By selecting features with high information gain, the decision tree
algorithm aims to create subsets that are more homogeneous or pure in
terms of the class labels. This allows for better classification or
prediction within each subset.

This method requires the continuous valued attributes to be discritized,
so we will discretize age, avg_glucose_level, and bmi. The method
doesn't accept char values, so we will change hypertension,
heart_disease, ever_married, work_type, and smoking_status:

```{r}
set.seed(123)
library('MASS')
data$age= cut(data$age, breaks= seq(0,90, by=40),right=TRUE)
data$avg_glucose_level	= cut(data$avg_glucose_level, breaks= seq(0,1, by=0.5),right=TRUE)
data$bmi	= cut(data$bmi, breaks= seq(0,1, by=0.5),right=TRUE)
data$hypertension <- as.factor(data$hypertension )
data$heart_disease <- as.factor(data$heart_disease )
data$ever_married   <- as.factor(data$ever_married  )
data$work_type   <- as.factor(data$work_type  )
data$smoking_status <- as.factor(data$smoking_status )
```

1-partition the data into ( 70% training, 30% testing):

```{r}
#splitting 70% of data for training, 30% of data for testing
set.seed(1234)
ind=sample (2, nrow(data), replace=TRUE, prob=c(0.70 , 0.30))
trainData=data[ind==1,]
testData=data[ind==2,]

#train using the trainData and create the ctree information gain tree
library(party)
library(plyr)
library(readr)
stroke_ctree <- ctree(myFormula, data=trainData)
print(stroke_ctree)
plot(stroke_ctree,type="simple")
plot(stroke_ctree)
```

```{r}
#make predictions using the ctree information gain tree on the test data
testPred <- predict(stroke_ctree, newdata = testData)
table(testPred, testData$stroke)

```

```{r}
##create confusion matrix to evaluate the model's performance
library(caret)
results <- confusionMatrix(testPred, testData$stroke)
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
```

2-partition the data into ( 80% training, 20% testing):

```{r}
#splitting 80% of data for training, 20% of data for testing
set.seed(1234)
ind=sample (2, nrow(data), replace=TRUE, prob=c(0.80 , 0.20))
trainData=data[ind==1,]
testData=data[ind==2,]

#train using the trainData and create the ctree information gain tree
library(party)
library(plyr)
library(readr)
stroke_ctree <- ctree(myFormula, data=trainData)
print(stroke_ctree)
plot(stroke_ctree,type="simple")
plot(stroke_ctree)
```

```{r}
#make predictions using the ctree information gain tree on the test data
testPred <- predict(stroke_ctree, newdata = testData)
table(testPred, testData$stroke)
```

```{r}
##create confusion matrix to evaluate the model's performance
library(caret)
results <- confusionMatrix(testPred, testData$stroke)
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
```

3-partition the data into ( 85% training, 15% testing):

```{r}
#splitting 85% of data for training, 15% of data for testing
set.seed(1234)
ind=sample (2, nrow(data), replace=TRUE, prob=c(0.85 , 0.15))
trainData=data[ind==1,]
testData=data[ind==2,]

#train using the trainData and create the ctree information gain tree
library(party)
library(plyr)
library(readr)
stroke_ctree <- ctree(myFormula, data=trainData)
print(stroke_ctree)
plot(stroke_ctree,type="simple")
plot(stroke_ctree)
```

```{r}
#make predictions using the ctree information gain tree on the test data
testPred <- predict(stroke_ctree, newdata = testData)
table(testPred, testData$stroke)
```

```{r}
##create confusion matrix to evaluate the model's performance
library(caret)
results <- confusionMatrix(testPred, testData$stroke)
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
```

-Gini index (rpart): In a nutshell, the Gini index is a measure of
impurity or the degree of disorder in a dataset. It is commonly used in
decision tree algorithms to evaluate the quality of a split when
constructing the tree.

When building a decision tree, each potential split is assessed using
the Gini index. The Gini index\
calculates the probability of misclassifying a randomly chosen element
from a dataset if it were randomly labeled according to the distribution
of classes in that subset. A lower Gini index indicates a more pure or
homogeneous subset, meaning that the classes within that subset are
similar.

To use the Gini index in a decision tree, the algorithm considers
various potential splits based on\
different features in the dataset. It calculates the Gini index for each
split and selects the one with the lowest value. The chosen split
results in the highest possible purity or homogeneity of the resulting
subsets.

1-partition the data into ( 70% training, 30% testing):

```{r}
#splitting 70% of data for training, 30% of data for testing
set.seed(1234)
ind=sample (2, nrow(data), replace=TRUE, prob=c(0.70 , 0.30))
trainData=data[ind==1,]
testData=data[ind==2,]

#train using the trainData and create the rpart gini index tree
library('rpart')
library('rpart.plot')
tree <- rpart(myFormula, data = trainData,method = 'class')
rpart.plot(tree)
```

```{r}
#make predictions using the rpart gini index tree on the test data
testPred <- predict(tree, newdata = testData,type = 'class')
table(testPred, testData$stroke)
```

```{r}
##create confusion matrix to evaluate the model's performance
library(caret)
results <- confusionMatrix(testPred, testData$stroke)
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
```

2-partition the data into ( 80% training, 20% testing):

```{r}
#splitting 80% of data for training, 20% of data for testing
set.seed(1234)
ind=sample (2, nrow(data), replace=TRUE, prob=c(0.80 , 0.20))
trainData=data[ind==1,]
testData=data[ind==2,]

#train using the trainData and create the rpart gini index tree
library('rpart')
library('rpart.plot')
tree <- rpart(myFormula, data = trainData,method = 'class')
rpart.plot(tree)
```

```{r}
#make predictions using the rpart gini index tree on the test data
testPred <- predict(tree, newdata = testData,type = 'class')
table(testPred, testData$stroke)
```

```{r}
##create confusion matrix to evaluate the model's performance
library(caret)
results <- confusionMatrix(testPred, testData$stroke)
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
```

3-partition the data into ( 85% training, 15% testing):

```{r}
#splitting 85% of data for training, 15% of data for testing
set.seed(1234)
ind=sample (2, nrow(data), replace=TRUE, prob=c(0.85 , 0.15))
trainData=data[ind==1,]
testData=data[ind==2,]

#train using the trainData and create the rpart gini index tree
library('rpart')
library('rpart.plot')
tree <- rpart(myFormula, data = trainData,method = 'class')
rpart.plot(tree)
```

```{r}
#make predictions using the rpart gini index tree on the test data
testPred <- predict(tree, newdata = testData,type = 'class')
table(testPred, testData$stroke)
```

```{r}
##create confusion matrix to evaluate the model's performance
library(caret)
results <- confusionMatrix(testPred, testData$stroke)
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
```
