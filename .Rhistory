labs(fill = "STROKE", title="Correlation between stroke occurence and marrige status")
# Histogram of Average Glucose Level with normal distribution overlay
histglucose <- hist(data$avg_glucose_level,xlim=c(0,300),
main="Histogram of Avg. Glucose with Normal Distribution Overlay",
xlab="Avg. Glucose",las=1)
xfit <- seq(min(data$avg_glucose_level),max(data$avg_glucose_level))
yfit <- dnorm(xfit,mean=mean(data$avg_glucose_level),sd=sd(data$avg_glucose_level))
yfit <- yfit*diff(histglucose$mids[1:2])*length(data$avg_glucose_level)
lines(xfit,yfit,col="red",lwd=2)
# Change "N/A" to actual NULL
data$bmi[data$bmi=="N/A"] <-NA
# Checking missing values
sum(is.na(data))
# Converting bmi to numeric
data$bmi <- as.numeric(as.character(data$bmi))
# Checking bmi type
class(data$bmi)
# Replacing null values with the mean
data$bmi[is.na(data$bmi)]<-mean(data$bmi, na.rm = TRUE)
# Missing values
sum(is.na(data))
# Checking duplicated rows
sum(duplicated(data))
#call outliers library
library(outliers)
#detect Age outliers
OutAge <- outlier(data$age)
print(OutAge)
#checking the outlier location before delete
indices <- which(data$age == OutAge)
# Print the resulting row indices
print(indices)
#Remove age outlier
data <- data[data$age != OutAge, ]
#detect Average glucose level outliers
OutAvg <- outlier(data$avg_glucose_level)
print(OutAvg)
#Remove Average glucose level outlier
data <- data[data$avg_glucose_level != OutAvg, ]
#detect bmi outliers
OutBMI <- outlier(data$bmi)
print(OutBMI)
#Remove bmi outlier
data <- data[data$bmi != OutBMI, ]
#check after deleting
#Number of rows
nrow(data)
#Number of column
ncol(data)
indices <- which(data$age == OutAge)
# Print the resulting row indices
print(indices)
indices3 <- which(data$avg_glucose_level == OutAvg)
# Print the resulting row indices
print(indices3)
indices2 <- which(data$bmi == 97.6)
# Print the resulting row indices
print(indices2)
#only one column with Gender "Other"
data[data$gender=="Other", ]
# delete it
data = data[data$gender!="Other", ]
##check
table(data$gender)
## convert stroke to factor(to use scaling)
data$stroke <- as.factor(data$stroke)
head(data)
data$work_type = factor(data$work_type,levels = c("Govt_job","Private", "Self-employed","children","Never_worked"), labels = c(5,4,3,2,1))
data$gender = factor(data$gender, levels = c("Male", "Female"), labels = c(1, 2))
data$ever_married= factor(data$ever_married, levels = c("No", "Yes"), labels = c(0, 1))
data$Residence_type= factor(data$Residence_type, levels = c("Urban", "Rural"), labels=c(1,2))
data$smoking_status= factor(data$smoking_status, levels = c("Unknown","never smoked", "formerly smoked","smokes"), labels=c(1,2,3,4))
head(data)
#normalize data
normalize <- function(x){ return ((x - min(x))/ (max(x)- min(x)))}
data$avg_glucose_level= normalize(data$avg_glucose_level)
data$age= normalize(data$age)
data$bmi= normalize(data$bmi)
head(data)
# ensure results are repeatable
set.seed(7)
# load the library
library(mlbench)
library(caret)
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
model <- train(stroke~., data, method="lvq", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(model, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)
# ensure the results are repeatable
set.seed(7)
# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(data[,1:11], data[,12], sizes=c(1:11), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
#delete the unimportant features coloumns
data <-data[,!names(data) %in% c("id","Residence_type","gender","heart_disease","bmi","work_type","smoking_status")]
head(data)
# upscaling the data
library('caret')
library('dplyr')
data<-upSample(data[,-5],data$stroke, yname="stroke")
plot(data$stroke)
data$stroke<- as.factor(data$stroke)
#Finally, checking overall
head(data)
#Number of rows
nrow(data)
## copying the data into 2 data frames, one for classification and one for clustering which will be used later
data_class <- data.frame(data)
data_cluster <- data.frame(data)
##define formula that takes stroke in relation to all the other features (easier to use later on)
myFormula <- stroke ~ age + hypertension +ever_married+avg_glucose_level
data_class <- data_class %>%group_by(stroke) %>% sample_n(size=500)
#splitting 70% of data for training, 30% of data for testing
library('C50')
library('caTools')
set.seed(1234)
sample <- sample.split(data_class$stroke, SplitRatio = 0.7)
trainData  <- subset(data_class, sample == TRUE)
testData   <- subset(data_class, sample == FALSE)
#train using the trainData and create the c5.0 gain ratio tree
strokeTree <- C5.0(myFormula, data=trainData)
summary(strokeTree)
plot(strokeTree)
#make predictions using the c5.0 gain ratio tree on the test data
testPred <- predict(strokeTree, newdata = testData)
#create confusion matrix to evaluate the model's performance
library(caret)
results <- confusionMatrix(testPred, testData$stroke, positive= "1")
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
#splitting 80% of data for training, 20% of data for testing
set.seed(1234)
sample <- sample.split(data_class$stroke, SplitRatio = 0.8)
trainData  <- subset(data_class, sample == TRUE)
testData   <- subset(data_class, sample == FALSE)
#train using the trainData and create the c5.0 gain ratio tree
strokeTree <- C5.0(myFormula, data=trainData)
summary(strokeTree)
plot(strokeTree)
#make predictions using the c5.0 gain ratio tree on the test data
testPred <- predict(strokeTree, newdata = testData)
#create confusion matrix to evaluate the model's performance
library(caret)
results <- confusionMatrix(testPred, testData$stroke, positive= "1")
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
#splitting 85% of data for training, 15% of data for testing
trctrl <- trainControl(method = "cv", number = 10, savePredictions=TRUE)
set.seed(123)
sample <- sample.split(data_class$stroke, SplitRatio = 0.85)
trainData  <- subset(data_class, sample == TRUE)
testData   <- subset(data_class, sample == FALSE)
#train using the trainData and create the c5.0 gain ratio tree
strokeTree <- C5.0(myFormula, data=trainData,)
summary(strokeTree)
plot(strokeTree)
#make predictions using the c5.0 gain ratio tree on the test data
testPred <- predict(strokeTree, newdata = testData)
#create confusion matrix to evaluate the model's performance
library(caret)
results <- confusionMatrix(testPred, testData$stroke, positive= "1")
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
#splitting 70% of data for training, 30% of data for testing
set.seed(123)
sample <- sample.split(data_class$stroke, SplitRatio = 0.7)
trainData  <- subset(data_class, sample == TRUE)
testData   <- subset(data_class, sample == FALSE)
#train using the trainData and create the rpart gini index tree
library('rpart')
library('rpart.plot')
tree <- rpart(myFormula, data = trainData,method = 'class')
rpart.plot(tree)
#make predictions using the rpart gini index tree on the test data
testPred <- predict(tree, newdata = testData,type = 'class')
##create confusion matrix to evaluate the model's performance
library(caret)
results <- confusionMatrix(testPred, testData$stroke, positive= "1")
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
#splitting 80% of data for training, 20% of data for testing
set.seed(123)
sample <- sample.split(data_class$stroke, SplitRatio = 0.8)
trainData  <- subset(data_class, sample == TRUE)
testData   <- subset(data_class, sample == FALSE)
#train using the trainData and create the rpart gini index tree
library('rpart')
library('rpart.plot')
tree <- rpart(myFormula, data = trainData,method = 'class')
rpart.plot(tree)
#make predictions using the rpart gini index tree on the test data
testPred <- predict(tree, newdata = testData,type = 'class')
##create confusion matrix to evaluate the model's performance
library(caret)
results <- confusionMatrix(testPred, testData$stroke, positive= "1")
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
#splitting 85% of data for training, 15% of data for testing
set.seed(123)
sample <- sample.split(data_class$stroke, SplitRatio = 0.85)
trainData  <- subset(data_class, sample == TRUE)
testData   <- subset(data_class, sample == FALSE)
#train using the trainData and create the rpart gini index tree
library('rpart')
library('rpart.plot')
tree <- rpart(myFormula, data = trainData,method = 'class')
rpart.plot(tree)
#make predictions using the rpart gini index tree on the test data
testPred <- predict(tree, newdata = testData,type = 'class')
##create confusion matrix to evaluate the model's performance
library(caret)
results <- confusionMatrix(testPred, testData$stroke, positive= "1")
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
set.seed(123)
library('MASS')
library("discretization")
cutPoints(data_class$age,data_class$stroke)
data_class$age= cut(data_class$age, breaks= seq(0,1, by=0.2),right=TRUE)
cutPoints(data_class$avg_glucose_level,data_class$stroke)
data_class$avg_glucose_level	= cut(data_class$avg_glucose_level, breaks= seq(0,1, by=0.5),right=TRUE)
data_class$hypertension <- as.factor(data_class$hypertension )
data_class$ever_married   <- as.factor(data_class$ever_married  )
#splitting 70% of data for training, 30% of data for testing
set.seed(123)
sample <- sample.split(data_class$stroke, SplitRatio = 0.7)
trainData  <- subset(data_class, sample == TRUE)
testData   <- subset(data_class, sample == FALSE)
#train using the trainData and create the ctree information gain tree
library(party)
library(plyr)
library(readr)
stroke_ctree <- ctree(myFormula, data=trainData)
print(stroke_ctree)
plot(stroke_ctree,type="simple")
plot(stroke_ctree)
#make predictions using the ctree information gain tree on the test data
testPred <- predict(stroke_ctree, newdata = testData)
##create confusion matrix to evaluate the model's performance
library(caret)
results <- confusionMatrix(testPred, testData$stroke, positive= "1")
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
#splitting 80% of data for training, 20% of data for testing
set.seed(123)
sample <- sample.split(data_class$stroke, SplitRatio = 0.8)
trainData  <- subset(data_class, sample == TRUE)
testData   <- subset(data_class, sample == FALSE)
#train using the trainData and create the ctree information gain tree
library(party)
library(plyr)
library(readr)
stroke_ctree <- ctree(myFormula, data=trainData)
print(stroke_ctree)
plot(stroke_ctree,type="simple")
plot(stroke_ctree)
#make predictions using the ctree information gain tree on the test data
testPred <- predict(stroke_ctree, newdata = testData)
##create confusion matrix to evaluate the model's performance
library(caret)
results <- confusionMatrix(testPred, testData$stroke, positive= "1")
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
#splitting 85% of data for training, 15% of data for testing
set.seed(123)
sample <- sample.split(data_class$stroke, SplitRatio = 0.85)
trainData  <- subset(data_class, sample == TRUE)
testData   <- subset(data_class, sample == FALSE)
#train using the trainData and create the ctree information gain tree
library(party)
library(plyr)
library(readr)
stroke_ctree <- ctree(myFormula, data=trainData)
print(stroke_ctree)
plot(stroke_ctree,type="simple")
plot(stroke_ctree)
#make predictions using the ctree information gain tree on the test data
testPred <- predict(stroke_ctree, newdata = testData)
##create confusion matrix to evaluate the model's performance
library(caret)
results <- confusionMatrix(testPred, testData$stroke, positive= "1")
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
library(factoextra)
sr<-(data_cluster$stroke)
data_cluster <- data_cluster[,!names(data_cluster) %in% c("stroke","id","Residence_type","gender","heart_disease","bmi","work_type","smoking_status")]
head(data_cluster)
str(data_cluster)
#Converting interger&factor columns too numeric
data_cluster$age<- as.numeric(data_cluster$age )
data_cluster$hypertension <- as.numeric(data_cluster$hypertension )
data_cluster$avg_glucose_level <- as.numeric(data_cluster$avg_glucose_level)
data_cluster$ever_married <- as.numeric(data_cluster$ever_married)
#for ease creating a data set without color column
data_no_color <- data_cluster[1:4]
#Let us see the structure again
str(data_no_color)
#######cluster k=2
#calculate k-mean k=2
km <- kmeans(data_cluster, 2, iter.max = 140 , algorithm="Lloyd", nstart=100)
#plot k-mean
fviz_cluster(list(data = data_cluster, cluster = km$cluster),
ellipse.type = "norm", geom = "point", stand = FALSE,
palette = "jco", ggtheme = theme_classic())
#avg silhouette
library(cluster)
sil <- silhouette(km$cluster, dist(data_cluster))
rownames(sil) <- rownames(data_cluster)
fviz_silhouette(sil)
# Total sum of squares
km$tot.withinss
cluster_assignments <- c(km$cluster)
ground_truth_labels <- c(data$stroke)
datacluster <- data.frame(cluster = cluster_assignments, label = ground_truth_labels)
# Function to calculate BCubed precision and recall
calculate_bcubed_metrics <- function(datacluster) {
n <- nrow(datacluster)
precision_sum <- 0
recall_sum <- 0
for (i in 1:n) {
cluster <- datacluster$cluster[i]
label <- datacluster$label[i]
# Count the number of items from the same category within the same cluster
same_category_same_cluster <- sum(datacluster$label[datacluster$cluster == cluster] == label)
# Count the total number of items in the same cluster
total_same_cluster <- sum(datacluster$cluster == cluster)
# Count the total number of items with the same category
total_same_category <- sum(datacluster$label == label)
# Calculate precision and recall for the current item and add them to the sums
precision_sum <- precision_sum + same_category_same_cluster /total_same_cluster
recall_sum <- recall_sum + same_category_same_cluster / total_same_category
}
# Calculate average precision and recall
precision <- precision_sum / n
recall <- recall_sum / n
return(list(precision = precision, recall = recall))
}
# Calculate BCubed precision and recall
metrics <- calculate_bcubed_metrics(datacluster)
# Extract precision and recall from the metrics
precision <- metrics$precision
recall <- metrics$recall
# Print the results
cat("BCubed Precision:", precision, "\n")
cat("BCubed Recall:", recall, "\n")
#######cluster k=3
#calculate k-mean
km <- kmeans(data_cluster, 3, iter.max = 140 , algorithm="Lloyd", nstart=100)
#plot k-mean
fviz_cluster(list(data = data_cluster, cluster = km$cluster),
ellipse.type = "norm", geom = "point", stand = FALSE,
palette = "jco", ggtheme = theme_classic())
#avg silhouette
library(cluster)
sil <- silhouette(km$cluster, dist(data_cluster))
rownames(sil) <- rownames(data_cluster)
fviz_silhouette(sil)
# Total sum of squares
km$tot.withinss
cluster_assignments <- c(km$cluster)
ground_truth_labels <- c(data$stroke)
datacluster <- data.frame(cluster = cluster_assignments, label = ground_truth_labels)
# Function to calculate BCubed precision and recall
calculate_bcubed_metrics <- function(datacluster) {
n <- nrow(datacluster)
precision_sum <- 0
recall_sum <- 0
for (i in 1:n) {
cluster <- datacluster$cluster[i]
label <- datacluster$label[i]
# Count the number of items from the same category within the same cluster
same_category_same_cluster <- sum(datacluster$label[datacluster$cluster == cluster] == label)
# Count the total number of items in the same cluster
total_same_cluster <- sum(datacluster$cluster == cluster)
# Count the total number of items with the same category
total_same_category <- sum(datacluster$label == label)
# Calculate precision and recall for the current item and add them to the sums
precision_sum <- precision_sum + same_category_same_cluster /total_same_cluster
recall_sum <- recall_sum + same_category_same_cluster / total_same_category
}
# Calculate average precision and recall
precision <- precision_sum / n
recall <- recall_sum / n
return(list(precision = precision, recall = recall))
}
# Calculate BCubed precision and recall
metrics <- calculate_bcubed_metrics(datacluster)
# Extract precision and recall from the metrics
precision <- metrics$precision
recall <- metrics$recall
# Print the results
cat("BCubed Precision:", precision, "\n")
cat("BCubed Recall:", recall, "\n")
#######cluster k=5
#calculate k-mean
km <- kmeans(data_cluster, 5, iter.max = 140 , algorithm="Lloyd", nstart=100)
#plot k-mean
fviz_cluster(list(data = data_cluster, cluster = km$cluster),
ellipse.type = "norm", geom = "point", stand = FALSE,
palette = "jco", ggtheme = theme_classic())
#avg silhouette
library(cluster)
sil <- silhouette(km$cluster, dist(data_cluster))
rownames(sil) <- rownames(data_cluster)
fviz_silhouette(sil)
# Total sum of squares
km$tot.withinss
cluster_assignments <- c(km$cluster)
ground_truth_labels <- c(data$stroke)
datacluster <- data.frame(cluster = cluster_assignments, label = ground_truth_labels)
# Function to calculate BCubed precision and recall
calculate_bcubed_metrics <- function(datacluster) {
n <- nrow(datacluster)
precision_sum <- 0
recall_sum <- 0
for (i in 1:n) {
cluster <- datacluster$cluster[i]
label <- datacluster$label[i]
# Count the number of items from the same category within the same cluster
same_category_same_cluster <- sum(datacluster$label[datacluster$cluster == cluster] == label)
# Count the total number of items in the same cluster
total_same_cluster <- sum(datacluster$cluster == cluster)
# Count the total number of items with the same category
total_same_category <- sum(datacluster$label == label)
# Calculate precision and recall for the current item and add them to the sums
precision_sum <- precision_sum + same_category_same_cluster /total_same_cluster
recall_sum <- recall_sum + same_category_same_cluster / total_same_category
}
# Calculate average precision and recall
precision <- precision_sum / n
recall <- recall_sum / n
return(list(precision = precision, recall = recall))
}
# Calculate BCubed precision and recall
metrics <- calculate_bcubed_metrics(datacluster)
# Extract precision and recall from the metrics
precision <- metrics$precision
recall <- metrics$recall
# Print the results
cat("BCubed Precision:", precision, "\n")
cat("BCubed Recall:", recall, "\n")
#elbow with wss
fviz_nbclust(data_cluster, kmeans, method = "wss")+ labs(subtitle = "Elbow method")
########### avg  silhouette for all cluster
fviz_nbclust(data_cluster, kmeans, method = "silhouette")
