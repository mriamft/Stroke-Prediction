rownames(sil) <- rownames(data_cluster)
fviz_silhouette(sil)
# Total sum of squares
km$tot.withinss
#bcubed metrix that take the avg of precision&recall
#install.packages('DPBBM')
library('DPBBM')
c = km$cluster
sr <- as.integer(sr)
BCubed_metric(sr, km$cluster, 0.5)
#######cluster k=5
#calculate k-mean
km <- kmeans(data_cluster, 5, iter.max = 140 , algorithm="Lloyd", nstart=100)
km
#plot k-mean
fviz_cluster(list(data = data_cluster, cluster = km$cluster),
ellipse.type = "norm", geom = "point", stand = FALSE,
palette = "jco", ggtheme = theme_classic())
#avg silhouette
library(cluster)
sil <- silhouette(km$cluster, dist(data_cluster))
rownames(sil) <- rownames(data_cluster)
fviz_silhouette(sil)
# Total sum of squares
km$tot.withinss
#bcubed metrix that take the avg of precision&recall
#install.packages('DPBBM')
library('DPBBM')
c = km$cluster
sr <- as.integer(sr)
BCubed_metric(sr, km$cluster, 0.5)
#elbow with wss
fviz_nbclust(data, kmeans, method = "wss")+ labs(subtitle = "Elbow method")
########### avg  silhouette for all cluster
fviz_nbclust(data, kmeans, method = "silhouette")
data<-read.csv("Dataset/healthcare-dataset-stroke-data.csv")
head(data)
str(data)
#Number of rows
nrow(data)
#Number of column
ncol(data)
# import necessary libraries
library(tidyverse)
library(ggplot2)
library(lubridate)
library(magrittr)
library(dplyr)
library(tidyr)
library(readr)
library(outliers)
library(caret)
library(DMwR2)
library(tidyverse)
library(lubridate)
library(skimr)
library(janitor)
library(dplyr)
library(tidyr)
library(readr)
library(ggplot2)
library(gridExtra)
# check gender
data <- data %>% filter(gender != "Other")
ggplot(data, aes(x = gender, fill = as.factor(stroke))) +
geom_bar(position = "fill") +
labs(fill = "STROKE", title= "Incident of stroke between genders")
tab <- data$work_type %>% table()
precentages <- tab %>% prop.table() %>% round(3) * 100
txt <- paste0(names(tab), '\n', precentages, '%') # text on chart
pie(tab, labels=txt ,main= "Chart of employement status") # plot pie chart
ggplot(data) + geom_point(mapping = aes(y = age, x = stroke, color = stroke ), alpha =0.9 )+
labs(title = "Distribution of Stroke status by age" ) +
theme( plot.title = element_text(size = 14, face = "bold"), legend.position = "none", axis.line = element_line(size = 1), axis.ticks = element_line() )
ggplot(data, aes(x = ever_married, fill = as.factor(stroke))) +
geom_bar(position = "fill") +
labs(fill = "STROKE", title="Correlation between stroke occurence and marrige status")
# Histogram of Average Glucose Level with normal distribution overlay
histglucose <- hist(data$avg_glucose_level,xlim=c(0,300),
main="Histogram of Avg. Glucose with Normal Distribution Overlay",
xlab="Avg. Glucose",las=1)
xfit <- seq(min(data$avg_glucose_level),max(data$avg_glucose_level))
yfit <- dnorm(xfit,mean=mean(data$avg_glucose_level),sd=sd(data$avg_glucose_level))
yfit <- yfit*diff(histglucose$mids[1:2])*length(data$avg_glucose_level)
lines(xfit,yfit,col="red",lwd=2)
# Change "N/A" to actual NULL
data$bmi[data$bmi=="N/A"] <-NA
# Checking missing values
sum(is.na(data))
# Converting bmi to numeric
data$bmi <- as.numeric(as.character(data$bmi))
# Checking bmi type
class(data$bmi)
# Replacing null values with the mean
data$bmi[is.na(data$bmi)]<-mean(data$bmi, na.rm = TRUE)
# Missing values
sum(is.na(data))
# Checking duplicated rows
sum(duplicated(data))
#call outliers library
library(outliers)
#detect Age outliers
OutAge <- outlier(data$age)
print(OutAge)
#checking the outlier location before delete
indices <- which(data$age == OutAge)
# Print the resulting row indices
print(indices)
#Remove age outlier
data <- data[data$age != OutAge, ]
#detect Average glucose level outliers
OutAvg <- outlier(data$avg_glucose_level)
print(OutAvg)
#Remove Average glucose level outlier
data <- data[data$avg_glucose_level != OutAvg, ]
#detect bmi outliers
OutBMI <- outlier(data$bmi)
print(OutBMI)
#Remove bmi outlier
data <- data[data$bmi != OutBMI, ]
#check after deleting
#Number of rows
nrow(data)
#Number of column
ncol(data)
# outliers row is removed now
print(data[, c("age", "avg_glucose_level","bmi")])
indices <- which(data$age == OutAge)
# Print the resulting row indices
print(indices)
indices3 <- which(data$avg_glucose_level == OutAvg)
# Print the resulting row indices
print(indices3)
indices2 <- which(data$bmi == 97.6)
# Print the resulting row indices
print(indices2)
#only one column with Gender "Other"
data[data$gender=="Other", ]
# delete it
data = data[data$gender!="Other", ]
##check
table(data$gender)
## convert stroke to factor(to use scaling)
data$stroke <- as.factor(data$stroke)
head(data)
data$work_type = factor(data$work_type,levels = c("Govt_job","Private", "Self-employed","children","Never_worked"), labels = c(5,4,3,2,1))
data$gender = factor(data$gender, levels = c("Male", "Female"), labels = c(1, 2))
data$ever_married= factor(data$ever_married, levels = c("No", "Yes"), labels = c(0, 1))
data$Residence_type= factor(data$Residence_type, levels = c("Urban", "Rural"), labels=c(1,2))
data$smoking_status= factor(data$smoking_status, levels = c("Unknown","never smoked", "formerly smoked","smokes"), labels=c(1,2,3,4))
head(data)
#normalize data
normalize <- function(x){ return ((x - min(x))/ (max(x)- min(x)))}
data$avg_glucose_level= normalize(data$avg_glucose_level)
data$age= normalize(data$age)
data$bmi= normalize(data$bmi)
head(data)
# ensure results are repeatable
set.seed(7)
# load the library
library(mlbench)
library(caret)
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
model <- train(stroke~., data, method="lvq", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(model, scale=FALSE)
# summarize importance
print(importance)
# plot importance
plot(importance)
# ensure the results are repeatable
set.seed(7)
# define the control using a random forest selection function
control <- rfeControl(functions=rfFuncs, method="cv", number=10)
# run the RFE algorithm
results <- rfe(data[,1:11], data[,12], sizes=c(1:11), rfeControl=control)
# summarize the results
print(results)
# list the chosen features
predictors(results)
# plot the results
plot(results, type=c("g", "o"))
#delete id,gender,residence type coloumns
data <-data[,!names(data) %in% c("id", "gender", "Residence_type")]
head(data)
# upscaling the data
library('caret')
library('dplyr')
data<-upSample(data[,-9],data$stroke, yname="stroke")
#data<-downSample(data[,-9],data$stroke, yname="stroke")
plot(data$stroke)
data$stroke<- as.factor(data$stroke)
## copying the data into 2 data frames, one for classification and one for clustering which will be used later
data_class <- data.frame(data)
data_cluster <- data.frame(data)
#Finally, checking overall
head(data)
#Number of rows
nrow(data)
myFormula <- stroke ~ age + hypertension +ever_married+avg_glucose_level
data_class <- data_class %>%group_by(stroke) %>% sample_n(size=500)
#splitting 70% of data for training, 30% of data for testing
library('C50')
set.seed(1234)
ind=sample (2, nrow(data_class), replace=TRUE, prob=c(0.70 , 0.30))
trainData=data_class[ind==1,]
testData=data_class[ind==2,]
#train using the trainData and create the c5.0 gain ratio tree
strokeTree <- C5.0(myFormula, data=trainData)
summary(strokeTree)
plot(strokeTree)
#make predictions using the c5.0 gain ratio tree on the test data
testPred <- predict(strokeTree, newdata = testData)
#create confusion matrix to evaluate the model's performance
library(caret)
results <- confusionMatrix(testPred, testData$stroke, positive= "1")
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
#splitting 80% of data for training, 20% of data for testing
set.seed(1234)
ind=sample (2, nrow(data_class), replace=TRUE, prob=c(0.80 , 0.20))
trainData=data_class[ind==1,]
testData=data_class[ind==2,]
#train using the trainData and create the c5.0 gain ratio tree
strokeTree <- C5.0(myFormula, data=trainData)
summary(strokeTree)
plot(strokeTree)
#make predictions using the c5.0 gain ratio tree on the test data
testPred <- predict(strokeTree, newdata = testData)
#create confusion matrix to evaluate the model's performance
library(caret)
results <- confusionMatrix(testPred, testData$stroke, positive= "1")
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
#splitting 85% of data for training, 15% of data for testing
trctrl <- trainControl(method = "cv", number = 10, savePredictions=TRUE)
set.seed(1234)
ind=sample (2, nrow(data_class), replace=TRUE, prob=c(0.85 , 0.15))
trainData=data_class[ind==1,]
testData=data_class[ind==2,]
#train using the trainData and create the c5.0 gain ratio tree
strokeTree <- C5.0(myFormula, data=trainData,)
summary(strokeTree)
plot(strokeTree)
#make predictions using the c5.0 gain ratio tree on the test data
testPred <- predict(strokeTree, newdata = testData)
#create confusion matrix to evaluate the model's performance
library(caret)
results <- confusionMatrix(testPred, testData$stroke, positive= "1")
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
set.seed(123)
library('MASS')
library("discretization")
cutPoints(data_class$age,data_class$stroke)
data_class$age= cut(data_class$age, breaks= seq(0,1, by=0.2),right=TRUE)
cutPoints(data_class$avg_glucose_level,data_class$stroke)
data_class$avg_glucose_level	= cut(data_class$avg_glucose_level, breaks= seq(0,1, by=0.5),right=TRUE)
cutPoints(data_class$bmi,data_class$stroke)
data_class$bmi	= cut(data_class$bmi, breaks= seq(0,0.7, by=0.2),right=TRUE)
data_class$hypertension <- as.factor(data_class$hypertension )
data_class$heart_disease <- as.factor(data_class$heart_disease )
data_class$ever_married   <- as.factor(data_class$ever_married  )
data_class$work_type   <- as.factor(data_class$work_type  )
data_class$smoking_status <- as.factor(data_class$smoking_status )
#splitting 70% of data for training, 30% of data for testing
set.seed(1234)
ind=sample (2, nrow(data_class), replace=TRUE, prob=c(0.70 , 0.30))
trainData=data_class[ind==1,]
testData=data_class[ind==2,]
#train using the trainData and create the ctree information gain tree
library(party)
library(plyr)
library(readr)
stroke_ctree <- ctree(myFormula, data=trainData)
print(stroke_ctree)
plot(stroke_ctree,type="simple")
plot(stroke_ctree)
#make predictions using the ctree information gain tree on the test data
testPred <- predict(stroke_ctree, newdata = testData)
##create confusion matrix to evaluate the model's performance
library(caret)
results <- confusionMatrix(testPred, testData$stroke, positive= "1")
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
#splitting 80% of data for training, 20% of data for testing
set.seed(321)
ind=sample (2, nrow(data_class), replace=TRUE, prob=c(0.80 , 0.20))
trainData=data_class[ind==1,]
testData=data_class[ind==2,]
#train using the trainData and create the ctree information gain tree
library(party)
library(plyr)
library(readr)
stroke_ctree <- ctree(myFormula, data=trainData)
print(stroke_ctree)
plot(stroke_ctree,type="simple")
plot(stroke_ctree)
#make predictions using the ctree information gain tree on the test data
testPred <- predict(stroke_ctree, newdata = testData)
##create confusion matrix to evaluate the model's performance
library(caret)
results <- confusionMatrix(testPred, testData$stroke, positive= "1")
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
#splitting 85% of data for training, 15% of data for testing
set.seed(111)
ind=sample (2, nrow(data_class), replace=TRUE, prob=c(0.85 , 0.15))
trainData=data_class[ind==1,]
testData=data_class[ind==2,]
#train using the trainData and create the ctree information gain tree
library(party)
library(plyr)
library(readr)
stroke_ctree <- ctree(myFormula, data=trainData)
print(stroke_ctree)
plot(stroke_ctree,type="simple")
plot(stroke_ctree)
#make predictions using the ctree information gain tree on the test data
testPred <- predict(stroke_ctree, newdata = testData)
##create confusion matrix to evaluate the model's performance
library(caret)
results <- confusionMatrix(testPred, testData$stroke, positive= "1")
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
#splitting 70% of data for training, 30% of data for testing
set.seed(1234)
ind=sample (2, nrow(data_class), replace=TRUE, prob=c(0.70 , 0.30))
trainData=data_class[ind==1,]
testData=data_class[ind==2,]
#train using the trainData and create the rpart gini index tree
library('rpart')
library('rpart.plot')
tree <- rpart(myFormula, data = trainData,method = 'class')
rpart.plot(tree)
#make predictions using the rpart gini index tree on the test data
testPred <- predict(tree, newdata = testData,type = 'class')
##create confusion matrix to evaluate the model's performance
library(caret)
results <- confusionMatrix(testPred, testData$stroke, positive= "1")
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
#splitting 80% of data for training, 20% of data for testing
set.seed(1234)
ind=sample (2, nrow(data_class), replace=TRUE, prob=c(0.80 , 0.20))
trainData=data_class[ind==1,]
testData=data_class[ind==2,]
#train using the trainData and create the rpart gini index tree
library('rpart')
library('rpart.plot')
tree <- rpart(myFormula, data = trainData,method = 'class')
rpart.plot(tree)
#make predictions using the rpart gini index tree on the test data
testPred <- predict(tree, newdata = testData,type = 'class')
##create confusion matrix to evaluate the model's performance
library(caret)
results <- confusionMatrix(testPred, testData$stroke, positive= "1")
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
#splitting 85% of data for training, 15% of data for testing
set.seed(1234)
ind=sample (2, nrow(data_class), replace=TRUE, prob=c(0.85 , 0.15))
trainData=data_class[ind==1,]
testData=data_class[ind==2,]
#train using the trainData and create the rpart gini index tree
library('rpart')
library('rpart.plot')
tree <- rpart(myFormula, data = trainData,method = 'class')
rpart.plot(tree)
#make predictions using the rpart gini index tree on the test data
testPred <- predict(tree, newdata = testData,type = 'class')
##create confusion matrix to evaluate the model's performance
library(caret)
results <- confusionMatrix(testPred, testData$stroke, positive= "1")
acc <- results$overall["Accuracy"]*100
acc
results
as.table(results)
as.matrix(results)
as.matrix(results, what = "overall")
as.matrix(results, what = "classes")
print(results)
library(factoextra)
sr<-(data_cluster$stroke)
data_cluster <- data_cluster[,!names(data_cluster) %in% c("stroke","id","Residence_type","gender","heart_disease","bmi","work_type","smoking_status")]
head(data_cluster)
str(data_cluster)
#Converting interger&factor columns too numeric
data_cluster$age<- as.numeric(data_cluster$age )
data_cluster$hypertension <- as.numeric(data_cluster$hypertension )
data_cluster$avg_glucose_level <- as.numeric(data_cluster$avg_glucose_level)
data_cluster$ever_married <- as.numeric(data_cluster$ever_married)
#for ease creating a data set without color column
data_no_color <- data_cluster[1:4]
#Let us see the structure again
str(data_no_color)
#######cluster k=2
#calculate k-mean k=2
km <- kmeans(data_cluster, 2, iter.max = 140 , algorithm="Lloyd", nstart=100)
km
#plot k-mean
fviz_cluster(list(data = data_cluster, cluster = km$cluster),
ellipse.type = "norm", geom = "point", stand = FALSE,
palette = "jco", ggtheme = theme_classic())
#avg silhouette
library(cluster)
sil <- silhouette(km$cluster, dist(data_cluster))
rownames(sil) <- rownames(data_cluster)
fviz_silhouette(sil)
# Total sum of squares
km$tot.withinss
#bcubed metrix that take the avg of precision&recall
library('DPBBM')
c = km$cluster
sr <- as.integer(sr)
BCubed_metric(sr, km$cluster, 0.5)
#######cluster k=3
#calculate k-mean
km <- kmeans(data_cluster, 3, iter.max = 140 , algorithm="Lloyd", nstart=100)
km
#plot k-mean
fviz_cluster(list(data = data_cluster, cluster = km$cluster),
ellipse.type = "norm", geom = "point", stand = FALSE,
palette = "jco", ggtheme = theme_classic())
#avg silhouette
library(cluster)
sil <- silhouette(km$cluster, dist(data_cluster))
rownames(sil) <- rownames(data_cluster)
fviz_silhouette(sil)
# Total sum of squares
km$tot.withinss
#bcubed metrix that take the avg of precision&recall
#install.packages('DPBBM')
library('DPBBM')
c = km$cluster
sr <- as.integer(sr)
BCubed_metric(sr, km$cluster, 0.5)
#######cluster k=5
#calculate k-mean
km <- kmeans(data_cluster, 5, iter.max = 140 , algorithm="Lloyd", nstart=100)
km
#plot k-mean
fviz_cluster(list(data = data_cluster, cluster = km$cluster),
ellipse.type = "norm", geom = "point", stand = FALSE,
palette = "jco", ggtheme = theme_classic())
#avg silhouette
library(cluster)
sil <- silhouette(km$cluster, dist(data_cluster))
rownames(sil) <- rownames(data_cluster)
fviz_silhouette(sil)
# Total sum of squares
km$tot.withinss
#bcubed metrix that take the avg of precision&recall
#install.packages('DPBBM')
library('DPBBM')
c = km$cluster
sr <- as.integer(sr)
BCubed_metric(sr, km$cluster, 0.5)
#elbow with wss
fviz_nbclust(data, kmeans, method = "wss")+ labs(subtitle = "Elbow method")
########### avg  silhouette for all cluster
fviz_nbclust(data, kmeans, method = "silhouette")
